{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "main.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "XNXblyIDh3dA",
        "F7hj09JAh9kL",
        "QOwWb1Y4iOrX",
        "GB6QOH_PixOj",
        "PnRQnj5yilXX",
        "MojRAyDRiJ7y",
        "gF-N6T2RqaT7",
        "FYOcdi_Eqh9R",
        "j6HCCu8yq1gx",
        "jJnkK945pkQI",
        "OwNTUQE_pxxF",
        "SXNPbYNfxX_1",
        "cvv-1k7JJY38"
      ],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXAYAnsI6VRt",
        "colab_type": "text"
      },
      "source": [
        "Introduction to Deep Learning - BGU - SISE\n",
        "Assignment 3 - From Melodies to Lyrics\n",
        "\n",
        "Submitted by Elior Nehemya & Eden Levy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNXblyIDh3dA",
        "colab_type": "text"
      },
      "source": [
        "# 0. Imports & Defs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVLHn-1Sh1ia",
        "colab_type": "code",
        "outputId": "95bdbb22-e189-4890-bff1-0563e642c094",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# Misc\n",
        "from time import time\n",
        "import itertools\n",
        "import datetime\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "# NLP\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "## from nltk.corpus import stopwords\n",
        "\n",
        "# SKLearn\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, confusion_matrix, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Keras\n",
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras.backend as K\n",
        "from keras.optimizers import Adadelta, Adam, SGD\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "# Word2Vec\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "\n",
        "# Google\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "ORIG_PATH = 'drive/My Drive/Colab Notebooks/Deep Learning - gilad/Assignment 3'\n",
        "if os.getcwd().find(ORIG_PATH) == -1: os.chdir(ORIG_PATH)\n",
        "PATH = './'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "5iDhkcYVh1ie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_to_word_list(text):\n",
        "    ''' Pre process and convert texts to a list of words '''\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^&,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "\n",
        "    text = nltk.word_tokenize(text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def concat_lyrics(df):\n",
        "    for idx, row in df.iterrows():\n",
        "        for i in range(3,7):\n",
        "            if len(row[i]) > 0:\n",
        "                row[2] = row[2] + ' & ' + row[i]\n",
        "        row[2] = text_to_word_list(row[2])\n",
        "        \n",
        "    df = df.drop(columns=range(3,7))\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7hj09JAh9kL",
        "colab_type": "text"
      },
      "source": [
        "# 1. Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AYWn3eZh1ig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_lyrics = pd.read_csv('./lyrics_train_set.csv', header=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "iSO9Kvkxh1ii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_lyrics = df_lyrics.fillna('')\n",
        "df_lyrics = concat_lyrics(df_lyrics)\n",
        "df_lyrics.columns = ['artist', 'title', 'lyrics']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "O96zrnkuh1ip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_feats = pd.read_csv('./features.csv')\n",
        "df_feats['artist'] = ''\n",
        "df_feats['title'] = ''\n",
        "\n",
        "# Transform the path into (artist, title)\n",
        "for idx, row in df_feats.iterrows():\n",
        "    info = row['Unnamed: 0'].split('\\\\')[-1].replace('.mid', '').replace('_', ' ').split(' - ')[:2]\n",
        "    df_feats.at[idx, 'artist'] = info[0].lower()\n",
        "    df_feats.at[idx, 'title'] = info[1].lower()\n",
        "    \n",
        "# \"depeche mode - enjoy the silence\" shows with -2\n",
        "# Code here\n",
        "\n",
        "# Drop filepath feature\n",
        "df_feats = df_feats.drop(columns=['Unnamed: 0'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "ceq0aNr5h1ir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inner join between midi features & lyrics\n",
        "df_merged = pd.merge(df_feats, df_lyrics, on=['artist', 'title'], how='inner')\n",
        "# Drop duplicate songs (only 1)\n",
        "df_merged = df_merged.drop_duplicates(subset=['artist', 'title'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOwWb1Y4iOrX",
        "colab_type": "text"
      },
      "source": [
        "# 2. Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB6QOH_PixOj",
        "colab_type": "text"
      },
      "source": [
        "## Local W2V"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC3yBUgCh1i8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare lyric data for w2v training\n",
        "w2v_train = []\n",
        "words_set = set()\n",
        "\n",
        "for song in df_merged['lyrics'].values:\n",
        "    w2v_train.append(song)\n",
        "    words_set |= {word for word in song}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "sFH9Mavch1i-",
        "colab_type": "code",
        "outputId": "7a7c7955-b0fc-47ed-bcb5-e3af2a9c84fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "word2vec_model = Word2Vec(w2v_train, size=300, window=5, min_count=1, workers=10)\n",
        "vocab_len = len(word2vec_model.wv.vocab)\n",
        "print('Word2Vec vocabulary size: {}'.format(vocab_len))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec vocabulary size: 7227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "yOh5Fkxhh1jB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_list = list(words_set)\n",
        "words2idx = {words_list[i] : i for i in range(len(words_list))}\n",
        "idx2word = {i : words_list[i] for i in range(len(words_list))}\n",
        "\n",
        "def get_one_hot(word_list):\n",
        "    integer_encoded = [words2idx[word_list[i]] for i in range(len(word_list))]\n",
        "    return to_categorical(integer_encoded, num_classes=vocab_len)\n",
        "\n",
        "\n",
        "def invers_one_hot(one_hot_list):\n",
        "    integer_encoded = argmax(one_hot_list, axis = -1)\n",
        "    return [idx2word[integer_encoded[i]] for i in range(len(one_hot_list))]\n",
        "\n",
        "\n",
        "def sample_probs(probs):\n",
        "    idx = np.random.choice(list(range(len(probs))), p=probs.ravel())\n",
        "    return idx2word[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnRQnj5yilXX",
        "colab_type": "text"
      },
      "source": [
        "## External W2V"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A3ua_9kh1ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##############################################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "OSn4aZEFh1iz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from gensim.models.word2vec import Word2Vec\n",
        "# from gensim.models import KeyedVectors\n",
        "# model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "NwbOIgnzh1i2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lyrics_word_set = set()\n",
        "# for idx, row in df_merged.iterrows():\n",
        "#     lyrics_word_set |= {word for word in row['lyrics']}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "P018AegSh1i4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# i=0\n",
        "# unk_words = []\n",
        "# for word in lyrics_word_set:\n",
        "#     if word not in model.vocab:\n",
        "#         i = i+1\n",
        "#         unk_words.append(word)\n",
        "\n",
        "# ctr = 0\n",
        "# for idx, row in df_merged.iterrows():\n",
        "#     for word in row['lyrics']:\n",
        "#         if word in unk_words:\n",
        "#             ctr = ctr + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "IECXaikZh1i6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###############################################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MojRAyDRiJ7y",
        "colab_type": "text"
      },
      "source": [
        "# 3. Midi Features Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FHKG3JUh1jD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np_midifeats = df_merged.drop(columns=['artist', 'lyrics', 'title']).values\n",
        "\n",
        "# Weird stringified NaN values\n",
        "np_midifeats[np.where(np_midifeats == ' NaN')] = .0\n",
        "\n",
        "# Normalize Data - can be any scaler\n",
        "scaler = MinMaxScaler()\n",
        "midifeats_scaled = scaler.fit_transform(np_midifeats)\n",
        "\n",
        "num_midifeats = midifeats_scaled.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF-N6T2RqaT7",
        "colab_type": "text"
      },
      "source": [
        "# 4. Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJLY7dYoLt6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Configs\n",
        "window_size = 5\n",
        "embedding_size = word2vec_model.vector_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1kd9xfxh1jU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = []\n",
        "y_train = []\n",
        "X_feat_train = []\n",
        "\n",
        "for idx, song in enumerate(w2v_train):\n",
        "  \n",
        "    # pad beginning of songs\n",
        "    song = list(('<UNK>',)*(window_size-1)) + song\n",
        "\n",
        "    for i in range(0,len(song)-window_size,1):\n",
        "        X_train.append([np.zeros((word2vec_model.vector_size,)) if song[i+j]=='<UNK>' else word2vec_model.wv[song[i+j]] for j in range(window_size)])\n",
        "        X_feat_train.append(midifeats_scaled[idx])\n",
        "        y_train.append(song[i+window_size])\n",
        "        \n",
        "X_train = np.array(X_train).reshape((len(X_train),window_size,300))\n",
        "X_feat_train = np.array(X_feat_train)\n",
        "y_train = np.array(y_train)\n",
        "y_train_one_hot = get_one_hot(y_train)\n",
        "y_train_one_hot.shape\n",
        "\n",
        "assert y_train_one_hot.shape == (X_train.shape[0], vocab_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYOcdi_Eqh9R",
        "colab_type": "text"
      },
      "source": [
        "# 5. Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "arfcNKJPh1jZ",
        "colab_type": "code",
        "outputId": "c3122338-0733-4a38-e4e8-4d339cacf2c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "def get_model_1():\n",
        "    inp_lyrics = Input((window_size, embedding_size), name='lyric_embeddings')\n",
        "\n",
        "    inp_midifeats = Input((num_midifeats,), name='midi_features')\n",
        "    duplicated_midifeats = concatenate(list((inp_midifeats,)*window_size), axis=1)\n",
        "    reshaped = Reshape((window_size, num_midifeats))(duplicated_midifeats)\n",
        "\n",
        "    full_input = concatenate([inp_lyrics, reshaped], axis=2)\n",
        "\n",
        "    l = CuDNNLSTM(256, return_sequences=True)(full_input)\n",
        "    l = CuDNNLSTM(128, return_sequences=False)(l)\n",
        "    fc = Dense(1024)(l)\n",
        "\n",
        "    out = Dense(vocab_len, activation='softmax')(fc)\n",
        "\n",
        "    model = Model([inp_lyrics, inp_midifeats], out)\n",
        "    optimizer = Adam()\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "model = get_model_1()\n",
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0805 14:21:12.091784 139742920800128 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0805 14:21:12.146513 139742920800128 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0805 14:21:13.862171 139742920800128 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0805 14:21:14.270575 139742920800128 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0805 14:21:14.278253 139742920800128 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "midi_features (InputLayer)      (None, 1495)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 7475)         0           midi_features[0][0]              \n",
            "                                                                 midi_features[0][0]              \n",
            "                                                                 midi_features[0][0]              \n",
            "                                                                 midi_features[0][0]              \n",
            "                                                                 midi_features[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lyric_embeddings (InputLayer)   (None, 5, 300)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 5, 1495)      0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 5, 1795)      0           lyric_embeddings[0][0]           \n",
            "                                                                 reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "cu_dnnlstm_1 (CuDNNLSTM)        (None, 5, 256)       2102272     concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "cu_dnnlstm_2 (CuDNNLSTM)        (None, 128)          197632      cu_dnnlstm_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1024)         132096      cu_dnnlstm_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 7227)         7407675     dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 9,839,675\n",
            "Trainable params: 9,839,675\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6HCCu8yq1gx",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzb6Rpe5yGDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tbCallBack = TensorBoard(log_dir='./log', histogram_freq=0,\n",
        "                         write_graph=True,\n",
        "                         batch_size=32,\n",
        "                         write_images=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmIG--Vmh1jb",
        "colab_type": "code",
        "outputId": "e586a537-07c7-4d7e-9794-a41370840961",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "history = model.fit(x=[X_train, X_feat_train], y=y_train_one_hot, epochs=10, verbose=1, callbacks=[tbCallBack], validation_split=0.1, batch_size=32)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0805 14:21:32.264303 139742920800128 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0805 14:21:32.526704 139742920800128 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 172123 samples, validate on 19125 samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0805 14:21:36.501655 139742920800128 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "W0805 14:21:36.503293 139742920800128 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "172123/172123 [==============================] - 81s 472us/step - loss: 5.2113 - acc: 0.1755 - val_loss: 5.2214 - val_acc: 0.1845\n",
            "Epoch 2/10\n",
            "172123/172123 [==============================] - 77s 450us/step - loss: 4.4854 - acc: 0.2331 - val_loss: 5.2439 - val_acc: 0.1900\n",
            "Epoch 3/10\n",
            "172123/172123 [==============================] - 78s 451us/step - loss: 4.0472 - acc: 0.2793 - val_loss: 5.2846 - val_acc: 0.1912\n",
            "Epoch 4/10\n",
            "172123/172123 [==============================] - 78s 453us/step - loss: 3.6896 - acc: 0.3215 - val_loss: 5.4534 - val_acc: 0.1868\n",
            "Epoch 5/10\n",
            "172123/172123 [==============================] - 78s 451us/step - loss: 3.3892 - acc: 0.3588 - val_loss: 5.5801 - val_acc: 0.1776\n",
            "Epoch 6/10\n",
            "172123/172123 [==============================] - 77s 449us/step - loss: 3.1416 - acc: 0.3913 - val_loss: 5.6945 - val_acc: 0.1826\n",
            "Epoch 7/10\n",
            "172123/172123 [==============================] - 78s 451us/step - loss: 2.9244 - acc: 0.4205 - val_loss: 5.8312 - val_acc: 0.1759\n",
            "Epoch 8/10\n",
            "172123/172123 [==============================] - 78s 452us/step - loss: 2.7374 - acc: 0.4452 - val_loss: 6.0576 - val_acc: 0.1643\n",
            "Epoch 9/10\n",
            "172123/172123 [==============================] - 78s 451us/step - loss: 2.5805 - acc: 0.4674 - val_loss: 6.1641 - val_acc: 0.1701\n",
            "Epoch 10/10\n",
            "172123/172123 [==============================] - 78s 451us/step - loss: 2.4371 - acc: 0.4882 - val_loss: 6.3495 - val_acc: 0.1638\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJnkK945pkQI",
        "colab_type": "text"
      },
      "source": [
        "# 6. Advanced Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVuB3mbfprbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model_2():\n",
        "\n",
        "    ########## Autoencoder Part ############\n",
        "\n",
        "    nb_epoch = 50\n",
        "    batch_size = 64\n",
        "\n",
        "    # Input dimension size (first and last autoencoder layer size)\n",
        "    input_dim = midifeats_scaled.shape[1]\n",
        "\n",
        "    # Setting the auto encoder layers\n",
        "    input_layer = Input(shape=(input_dim, ))\n",
        "\n",
        "    encoder_l1 = Dense(256, activation=\"relu\")(input_layer) \n",
        "    encoder_d1 = Dropout(0.2)(encoder_l1)\n",
        "    encoder_l2 = Dense(64, activation=\"relu\")(encoder_d1) \n",
        "    encoder = Dropout(0.2)(encoder_l2)\n",
        "    decoder = Dense(256, activation=\"relu\")(encoder)\n",
        "\n",
        "    decoder = Dense(input_dim, activation=\"sigmoid\")(decoder) \n",
        "\n",
        "    ae = Model(inputs=input_layer, outputs=decoder)\n",
        "    \n",
        "    ae_history = train_ae(ae)\n",
        "\n",
        "    ################ NN ####################\n",
        "\n",
        "    inp_midifeats = Input((num_midifeats,))\n",
        "\n",
        "    # Encode midi features input using pre-trained layers\n",
        "    enc_l1 = Dense(256, activation=\"relu\", name=\"encoder_l1\")(inp_midifeats) \n",
        "    encoded = Dense(64, activation=\"relu\", name=\"encoder_l2\")(enc_l1)\n",
        "\n",
        "    # Copy features for each timestamp in LSTM\n",
        "    duplicated_midifeats = concatenate(list((encoded,)*window_size), axis=1)\n",
        "    reshaped = Reshape((window_size, 64))(duplicated_midifeats)\n",
        "\n",
        "    inp_lyrics = Input((window_size, embedding_size))\n",
        "\n",
        "    full_input = concatenate([inp_lyrics, reshaped], axis=2)\n",
        "\n",
        "    l = CuDNNLSTM(256, return_sequences=True)(full_input)\n",
        "    l = CuDNNLSTM(128, return_sequences=False)(l)\n",
        "    fc = Dense(1024)(l)\n",
        "\n",
        "    out = Dense(vocab_len, activation='softmax')(fc)\n",
        "\n",
        "    model = Model([inp_lyrics, inp_midifeats], out)\n",
        "    optimizer = Adam()\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Set pre-trained weights for the encoder part\n",
        "    l1_w, l1_b, l2_w, l2_b = ae.get_weights()[:4]\n",
        "    model.get_layer('encoder_l1').set_weights([l1_w, l1_b])\n",
        "    model.get_layer('encoder_l2').set_weights([l2_w, l2_b])\n",
        "\n",
        "    model.get_layer('encoder_l1').trainable = False\n",
        "    model.get_layer('encoder_l2').trainable = False\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_ae(ae, nb_epoch=64, batch_size=64, val_split=0.1):\n",
        "    nb_epoch = 200\n",
        "    batch_size = 64\n",
        "\n",
        "    # Configures the learning process of the network\n",
        "    ae.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
        "\n",
        "    # Train the autoencoder based on the best epoch, returns history object\n",
        "    history = ae.fit(midifeats_scaled, midifeats_scaled, epochs=nb_epoch, batch_size=batch_size, verbose=0, validation_split=0.1)\n",
        "\n",
        "    return history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J24upYRIprdi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "ca44d4c8-21ca-432f-f046-8e5cdea503b2"
      },
      "source": [
        "model_2 = get_model_2()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0805 13:15:01.131497 140526145329024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0805 13:15:01.139565 140526145329024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0805 13:15:01.161312 140526145329024 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0805 13:15:01.223661 140526145329024 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwNTUQE_pxxF",
        "colab_type": "text"
      },
      "source": [
        "## 7. Training 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5IlJKCnp0ml",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "6ccc5a5c-660e-4338-b18b-73ec927b74e1"
      },
      "source": [
        "tbCallBack = TensorBoard(log_dir='./log', histogram_freq=0,\n",
        "                         write_graph=True,\n",
        "                         write_grads=True,\n",
        "                         batch_size=32,\n",
        "                         write_images=True)\n",
        "history_2 = model_2.fit(x=[X_train, X_feat_train], y=y_train_one_hot, epochs=10, verbose=1, validation_split=0.1\n",
        "                        ,callbacks=[tbCallBack]\n",
        "                        )"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 172123 samples, validate on 19125 samples\n",
            "Epoch 1/10\n",
            "172123/172123 [==============================] - 69s 400us/step - loss: 4.4833 - acc: 0.2318 - val_loss: 5.1603 - val_acc: 0.1971\n",
            "Epoch 2/10\n",
            "172123/172123 [==============================] - 69s 401us/step - loss: 4.0898 - acc: 0.2717 - val_loss: 5.1902 - val_acc: 0.1993\n",
            "Epoch 3/10\n",
            "172123/172123 [==============================] - 69s 401us/step - loss: 3.7590 - acc: 0.3111 - val_loss: 5.3014 - val_acc: 0.2007\n",
            "Epoch 4/10\n",
            "172123/172123 [==============================] - 69s 401us/step - loss: 3.4683 - acc: 0.3464 - val_loss: 5.2979 - val_acc: 0.1983\n",
            "Epoch 5/10\n",
            "172123/172123 [==============================] - 69s 400us/step - loss: 3.2105 - acc: 0.3781 - val_loss: 5.4514 - val_acc: 0.1904\n",
            "Epoch 6/10\n",
            "172123/172123 [==============================] - 68s 397us/step - loss: 2.9831 - acc: 0.4091 - val_loss: 5.5276 - val_acc: 0.1887\n",
            "Epoch 7/10\n",
            "172123/172123 [==============================] - 69s 400us/step - loss: 2.7884 - acc: 0.4365 - val_loss: 5.6238 - val_acc: 0.1932\n",
            "Epoch 8/10\n",
            "172123/172123 [==============================] - 69s 402us/step - loss: 2.6071 - acc: 0.4623 - val_loss: 5.6877 - val_acc: 0.1838\n",
            "Epoch 9/10\n",
            "172123/172123 [==============================] - 69s 401us/step - loss: 2.4543 - acc: 0.4843 - val_loss: 5.9271 - val_acc: 0.1741\n",
            "Epoch 10/10\n",
            "172123/172123 [==============================] - 69s 400us/step - loss: 2.3137 - acc: 0.5059 - val_loss: 5.9280 - val_acc: 0.1839\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXNPbYNfxX_1",
        "colab_type": "text"
      },
      "source": [
        "# 7. Test Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vVfLoasNzgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I6VONw-h1ji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_song_len = sum(df_merged['lyrics'].apply(len)) / df_merged.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq_agrQyw_sk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def query_model(model, word, midi_vector):\n",
        "\n",
        "    # Prepare lyrics input\n",
        "    inp_lyrics = np.zeros((1, window_size, embedding_size))\n",
        "    inp_lyrics[0, window_size-1,:] = word2vec_model.wv[word]\n",
        "\n",
        "    # Prepare midi input\n",
        "    midi_vector = midi_vector\n",
        "\n",
        "    predicted_song = [word]\n",
        "    i = 0\n",
        "    while (i < avg_song_len):\n",
        "        preds = model.predict([inp_lyrics, midi_vector])\n",
        "        pred_word = sample_probs(preds[0])\n",
        "        \n",
        "        predicted_song.append(pred_word)\n",
        "\n",
        "        inp_lyrics = np.roll(inp_lyrics, -1, axis=1)\n",
        "        inp_lyrics[0, window_size-1, :] = word2vec_model.wv[pred_word]\n",
        "\n",
        "        i = i + 1\n",
        "\n",
        "    forprint = ' '.join(predicted_song).replace(' & ', '\\n')\n",
        "\n",
        "    return forprint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3yaAJ888we8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read test data\n",
        "\n",
        "df_lyrics = pd.read_csv('./lyrics_test_set.csv', header=-1 )\n",
        "\n",
        "\n",
        "for idx, row in df_lyrics.iterrows():\n",
        "    row[2] = text_to_word_list(row[2])\n",
        "    row[1] = str(row[1])[1:]\n",
        "\n",
        "df_lyrics.columns = ['artist', 'title', 'lyrics']\n",
        "\n",
        "df_feats = pd.read_csv('./features.csv')\n",
        "df_feats['artist'] = ''\n",
        "df_feats['title'] = ''\n",
        "\n",
        "# Transform the path into (artist, title)\n",
        "for idx, row in df_feats.iterrows():\n",
        "    info = row['Unnamed: 0'].split('\\\\')[-1].replace('.mid', '').replace('_', ' ').split(' - ')[:2]\n",
        "    df_feats.at[idx, 'artist'] = info[0].lower()\n",
        "    df_feats.at[idx, 'title'] = info[1].lower()\n",
        "    \n",
        "\n",
        "# Drop filepath feature\n",
        "df_feats = df_feats.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "# Inner join between midi features & lyrics\n",
        "df_merged = pd.merge(df_feats, df_lyrics, on=['artist', 'title'], how='inner')\n",
        "# Drop duplicate songs (only 1)\n",
        "# df_merged = df_merged.drop_duplicates(subset=['artist', 'title'])\n",
        "\n",
        "np_midifeats = df_merged.drop(columns=['artist', 'lyrics', 'title']).values\n",
        "\n",
        "# Weird stringified NaN values\n",
        "np_midifeats[np.where(np_midifeats == ' NaN')] = .0\n",
        "\n",
        "# Normalize Data - can be any scaler\n",
        "scaler = MinMaxScaler()\n",
        "midifeats_scaled = scaler.fit_transform(np_midifeats)\n",
        "\n",
        "num_midifeats = midifeats_scaled.shape[1]\n",
        "\n",
        "\n",
        "# generate test songs:\n",
        "\n",
        "melodies = [midifeats_scaled[i:i+1] for i in range(5)]\n",
        "words = ['love', 'believe', 'rage' ]\n",
        "songs = []\n",
        "for melody in melodies:\n",
        "    for word in words:\n",
        "        songs.append(query_model(model, word, melody))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvoREp23KDyL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c5f31af5-2d01-4bf3-a38d-d283266123be"
      },
      "source": [
        "# TEST SONGS - LOAD PREMADE SONGS\n",
        "\n",
        "songs1 = np.load('model1_songs.npy')\n",
        "songs2 = np.load('model2_songs.npy')\n",
        "\n",
        "songs1 = [song.replace(' \\n ', '\\n') for song in songs1]\n",
        "songs2 = [song.replace(' \\n ', '\\n') for song in songs2]\n",
        "\n",
        "w = ['love', 'believe', 'rage']\n",
        "\n",
        "print('############ Songs generated by Basic Model #############')\n",
        "for i, song in enumerate(songs1):\n",
        "  print('*'*60)\n",
        "  print('Artist: ' + df_merged['artist'][int(i/3)])\n",
        "  print('Title: ' + df_merged['title'][int(i/3)])\n",
        "  print('Starting word: {}\\n'.format(w[i%3]))\n",
        "  print(song)\n",
        "\n",
        "  \n",
        "  \n",
        "print('############ Songs generated by Advanced Model #############')\n",
        "for i, song in enumerate(songs2):\n",
        "  print('*'*60)\n",
        "  print('Artist: ' + df_merged['artist'][int(i/3)])\n",
        "  print('Title: ' + df_merged['title'][int(i/3)])\n",
        "  print('Starting word: {}\\n'.format(w[i%3]))\n",
        "  print(song)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "############ Songs generated by Basic Model #############\n",
            "************************************************************\n",
            "Artist: aqua\n",
            "Title: barbie girl\n",
            "Starting word: love\n",
            "\n",
            "love start start bus suit sad shadow that find deep find sad sad sad surely that shadow must find find sad bus bus wo die find find find sad sad sad sad find start sad earl find walls start glad find find start die find sad sad die find deep shadow bus bus that find find die name die quick deep sad recognize earl find start find die find sad sad die deep find walls start sad die find find find find find find quick gentle sad find stop die die\n",
            "************************************************************\n",
            "Artist: aqua\n",
            "Title: barbie girl\n",
            "Starting word: believe\n",
            "\n",
            "believe bye fun earl earl shadow that find find die sad sad find sad sad find deep sad find walls quick bus that find find die die die quick special that find start sad sad sad sad find die shadow bus bus that find find die sad die sad sad sad my\n",
            "shadow different start sad life find sad sad bus must find find find find quick sad sad find stuff sad sad suit our must find walls die walls earl find find die die die quick special much find\n",
            "************************************************************\n",
            "Artist: aqua\n",
            "Title: barbie girl\n",
            "Starting word: rage\n",
            "\n",
            "rage had find start sad die sad sad sad find sad shadow missed\n",
            "die sad die walls sad sad must sad sad find longest find find find find find gentle find find find sad sad sad sad surely find start shadow shadow bus that must find find sad sad earl sad find find die earl bus sad suit find surely find die die name die quick deep sad my that shadow earl find sad sad find start sad find find quick find die sad die sad sad must my name\n",
            "************************************************************\n",
            "Artist: billy joel\n",
            "Title: honesty\n",
            "Starting word: love\n",
            "\n",
            "love\n",
            "open 23 create someday we\n",
            "oh la free\n",
            "only repeat if still in more more ever more wrong thinking more danger troup-p-per\n",
            "than would ever smart ugly more 'til cried capture c'mon die\n",
            "we down dancing hair angels hair blues troup-p-per listen blues c'mon ting information\n",
            "how me someday buy in old more man could could always more falling needs more danger loved\n",
            "open c'mon information cried\n",
            "ah walk still in a\n",
            "still gave danger loud in life\n",
            "ever fools\n",
            "************************************************************\n",
            "Artist: billy joel\n",
            "Title: honesty\n",
            "Starting word: believe\n",
            "\n",
            "believe\n",
            "still livin golden enough in\n",
            "still could never board ugly\n",
            "our old man in\n",
            "\n",
            " open only falling tellin falling\n",
            "no songs loud information needs capture ah\n",
            "games someday still saw in had ... more a more ever fought b memory heaven old apart times\n",
            "upon beyond impregnate nice people once\n",
            "never fun always wish wish in ...\n",
            "if never set set riot a hair\n",
            "follow meant loud c'mon\n",
            "say dead a friends gettin bones\n",
            "like filled 23\n",
            "************************************************************\n",
            "Artist: billy joel\n",
            "Title: honesty\n",
            "Starting word: rage\n",
            "\n",
            "rage\n",
            "apart prints someday\n",
            "the apart still walk son taught quick\n",
            "how clearly knowing nice\n",
            "how me memories two in oh golden upon\n",
            "a sister nose memory\n",
            "wrong have the daddy in more more affecting wrong more danger ahead knocked\n",
            "how me coal needs two than\n",
            "holding loud loud nice\n",
            "once hide die needs in\n",
            "oh yes\n",
            "a 'd all only could ever black drag taught\n",
            "how me a child takin cried hair\n",
            "oh\n",
            "a ... in\n",
            "************************************************************\n",
            "Artist: blink 182\n",
            "Title: all the small things\n",
            "Starting word: love\n",
            "\n",
            "love an ya this this i nothing rhythm people old this rest talk this water ha ha ha ha c'mon run run ha ha ha ha run run run ha ha ha run run run ha ha ha ha run run run ha ha ha ha run run run ha ha ha ha ha ha run run run ha ha ha ha run run run ha ha ha ha run run run ha ha ha ha run run run ha ha ha ha run run run ha ha ha ha run\n",
            "************************************************************\n",
            "Artist: blink 182\n",
            "Title: all the small things\n",
            "Starting word: believe\n",
            "\n",
            "believe this stayin this old old people water hills since since since ha ha great run run ha ha ha ha run run run ha ha ha ha run run run ha ha ha ha run run run run ha ha ha ha ha ha ha ha ha run run run ha ha ha ha run run run ha ha ha run run run ha ha ha ha run run run ha ha ha ha run run run ha ha ha ha run run run ha ha ha ha ha ha\n",
            "************************************************************\n",
            "Artist: blink 182\n",
            "Title: all the small things\n",
            "Starting word: rage\n",
            "\n",
            "rage this an goodbye this this rhythm rhythm old born this stayin this since this early this early water old this this over this nothing late damned this found blues this smoke blues this ha blues ha ha ha ha c'mon run run ha run ha ha ha run run run ha ha ha ha ha run run run ha ha ha ha run run run ha ha ha ha run run run ha ha ha ha run run run ha ha ha ha ha ha ha run run run ha\n",
            "************************************************************\n",
            "Artist: cardigans\n",
            "Title: lovefool\n",
            "Starting word: love\n",
            "\n",
            "love minutes bear talk it it help talk part egyptian dee bye stayin sleigh sleigh wiggy dee dee sleigh sleigh egyptian molasses clearly shu-bop shu-bop egyptian minutes shu-bop shu-bop egyptian shu-bop icy albums dee eight egyptian egyptian egyptian egyptian shu-bop egyptian shu-bop egyptian shu-bop shu-bop icy shu-bop egyptian dee egyptian egyptian sleigh egyptian egyptian shu-bop shu-bop sears shu-bop shu-bop sleigh egyptian shu-bop dee stayin sleigh sleigh meter dee freak sleigh sleigh egyptian clearly dee eight sleigh egyptian shu-bop clearly shu-bop dee eight swerve shu-bop shu-bop clearly shu-bop shu-bop sleigh sleigh bye\n",
            "************************************************************\n",
            "Artist: cardigans\n",
            "Title: lovefool\n",
            "Starting word: believe\n",
            "\n",
            "believe shu-bop egyptian shu-bop dee bye sleigh sleigh egyptian clearly dee eight egyptian marry egyptian cream shu-bop egyptian shu-bop egyptian shu-bop shu-bop icy shu-bop shu-bop sleigh icy dee eight sleigh sleigh egyptian shu-bop icy bye shu-bop egyptian meter egyptian albums shu-bop egyptian shu-bop egyptian shu-bop shu-bop egyptian meter shu-bop sleigh egyptian shu-bop shu-bop sleigh sleigh sleigh sleigh sleigh sleigh sleigh icy dee bye sleigh sleigh egyptian shu-bop sleigh pinky meter shu-bop shu-bop icy egyptian shu-bop shu-bop icy shu-bop shu-bop sleigh icy minutes clearly shu-bop shu-bop egyptian icy shu-bop sooner shu-bop egyptian\n",
            "************************************************************\n",
            "Artist: cardigans\n",
            "Title: lovefool\n",
            "Starting word: rage\n",
            "\n",
            "rage ride capture egyptian egyptian egyptian shu-bop egyptian shu-bop egyptian shu-bop weather icy clearly shu-bop shu-bop shu-bop egyptian minutes dee eight marry lurkin clearly icy ride ride egyptian sleigh meter meter dee egyptian egyptian lurkin marry clearly shu-bop ride egyptian egyptian shu-bop shu-bop icy shu-bop shu-bop sleigh sleigh icy bye clearly bye conductors shu-bop clearly egyptian dee egyptian egyptian egyptian egyptian shu-bop icy shu-bop egyptian shu-bop egyptian shu-bop shu-bop swerve shu-bop sleigh freak sleigh freak sleigh shot eight meter dee eight shu-bop egyptian virgin clearly shu-bop egyptian minutes shu-bop egyptian shu-bop\n",
            "************************************************************\n",
            "Artist: the bangles\n",
            "Title: eternal flame\n",
            "Starting word: love\n",
            "\n",
            "love evil women women change wherever\n",
            "mony\n",
            "fade\n",
            "lose die see fade turning face goin\n",
            "\n",
            " change\n",
            "lose die die duty roller huh broke fight train fight shore\n",
            "fade\n",
            "change change die die duty\n",
            "face awake lord start run fight fight\n",
            "go change\n",
            "fade run die bought die shore\n",
            "tenderly\n",
            "fade change\n",
            "change die stand face g\n",
            "goin awake\n",
            "change\n",
            "change\n",
            "lose die die duty dust warm morning shore\n",
            "\n",
            " change\n",
            "\n",
            "\n",
            "************************************************************\n",
            "Artist: the bangles\n",
            "Title: eternal flame\n",
            "Starting word: believe\n",
            "\n",
            "believe na na na tail without day change proud\n",
            "see see change\n",
            "see see see start see\n",
            "see see start see see see start\n",
            "\n",
            " see change start die die goin nowhere ' quit\n",
            "\n",
            " get see each see\n",
            "see\n",
            "see lose see see start\n",
            "\n",
            " see die start reach die goin run reach\n",
            "change paperlate\n",
            "change roller\n",
            "change morning singing\n",
            "must\n",
            "see change start\n",
            "die die mornin way bought secret\n",
            "change change train\n",
            "bad\n",
            "************************************************************\n",
            "Artist: the bangles\n",
            "Title: eternal flame\n",
            "Starting word: rage\n",
            "\n",
            "rage change change shore\n",
            "hangin . singing dead bought mony repeat train dance fight morning shore\n",
            "rules fade\n",
            "change singing\n",
            "fade names\n",
            "change earl\n",
            "change die singing names forget shore\n",
            "mony\n",
            "change change\n",
            "change die nowhere\n",
            "die city myself fight bought\n",
            "singing\n",
            "sign fade fade\n",
            "run shore\n",
            "fade fade\n",
            "change sign singing\n",
            "everywhere\n",
            "fade\n",
            "lose\n",
            "lose die die duty may fly awake repeat change singing train train shore\n",
            "wrote\n",
            "fade\n",
            "############ Songs generated by Advanced Model #############\n",
            "************************************************************\n",
            "Artist: aqua\n",
            "Title: barbie girl\n",
            "Starting word: love\n",
            "\n",
            "love me high are like\n",
            "everybody gives the broken\n",
            "then it 's true\n",
            "so i listen to you\n",
            "i 've got a cards face for you need and i do n't mind a\n",
            "still there 's somebody you\n",
            "yes i 'd still i\n",
            "chasing that you sugar until all\n",
            "faded all the blues\n",
            "love no know i live\n",
            "i do n't give your\n",
            "i if give a straight trying\n",
            "'cause i feel because\n",
            "to live until a man\n",
            "************************************************************\n",
            "Artist: aqua\n",
            "Title: barbie girl\n",
            "Starting word: believe\n",
            "\n",
            "believe where you are at a were\n",
            "what can it be wrong\n",
            "may it be gon na try like taking\n",
            "so has feelings\n",
            "i still alone clouds but now\n",
            "ride the sad\n",
            "i 'll hold somebody\n",
            "i 'm letting you till down\n",
            "ice\n",
            "tempted in you i want to give you still\n",
            "i 'm gon na find\n",
            "but now you know i could get away\n",
            "and tell me a soul to dance\n",
            "you gon na start the best\n",
            "************************************************************\n",
            "Artist: aqua\n",
            "Title: barbie girl\n",
            "Starting word: rage\n",
            "\n",
            "rage nowhere it about dream\n",
            "so\n",
            "never there we had to help\n",
            "together to you\n",
            "think i can live together\n",
            "here to heaven with\n",
            "but i do have to learn\n",
            "there 'd can play forever\n",
            "if i wrong when the way my life\n",
            "just caught your hand\n",
            "but there 's somebody\n",
            "and i will still wait\n",
            "lying\n",
            "tender has disappeared old midnight\n",
            "so i do have to let you tight\n",
            "say you i do give you\n",
            "************************************************************\n",
            "Artist: billy joel\n",
            "Title: honesty\n",
            "Starting word: love\n",
            "\n",
            "love 's late on your rising at the things you do from you\n",
            "stay me mend you to see why me it means when this life ago you were\n",
            "and there 's nothing never leave\n",
            "this folks again until you can tell me my eyes to see me something true to me\n",
            "waterloo us to remain before\n",
            "i saw the window again\n",
            "meet he being to each up true\n",
            "it does n't it just\n",
            "give me the spark again\n",
            "it really n't\n",
            "************************************************************\n",
            "Artist: billy joel\n",
            "Title: honesty\n",
            "Starting word: believe\n",
            "\n",
            "believe me there 'll mine someone light heaven time\n",
            "even it better we will explain hungry time\n",
            "time you left girl had too sad to me\n",
            "and i would n't hear to decide you 'll find\n",
            "you gave to give a sad\n",
            "relax you feel forget me help\n",
            "a thrill of hell that doubt in the wind in the night\n",
            "and i lay break darlin night hollywood i keep you keep i start it burns\n",
            "but in there weed that you say 's\n",
            "************************************************************\n",
            "Artist: billy joel\n",
            "Title: honesty\n",
            "Starting word: rage\n",
            "\n",
            "rage runs in a shining i left a person ring-a-ring-a-ring of my heart maybe the memory\n",
            "in the time in my place you had to see me what it is me something caught and you think of light the light are something as cats\n",
            "all i 'd n't even to to do though nobody to do my world it 's like a bird at my hands of the light\n",
            "all there\n",
            "i was friends the price of this way\n",
            "before i 'd faded my shit i\n",
            "************************************************************\n",
            "Artist: blink 182\n",
            "Title: all the small things\n",
            "Starting word: love\n",
            "\n",
            "love have ya from nothin\n",
            "from the girls on out\n",
            "the lady ai n't had\n",
            "they du anyone to think that shit\n",
            "and stop oo-hoo-hoo-oo-oo to fake\n",
            "fa always on your knees\n",
            "but do n't try anybody why they 've got married gon na be better they do n't think he decency me\n",
            "whom in the floors\n",
            "we roll you safe to cling\n",
            "ooh-ooh until the point can wonderful\n",
            "i keep the meter and\n",
            "some say they say in ya\n",
            "************************************************************\n",
            "Artist: blink 182\n",
            "Title: all the small things\n",
            "Starting word: believe\n",
            "\n",
            "believe do it war\n",
            "mic\n",
            "verse known for you can talk\n",
            "why come and act the yo\n",
            "chorus\n",
            "they tried away to the king\n",
            "chorus me sounding with the tune\n",
            "while fields a garden\n",
            "jealous boogie your old\n",
            "gives sure did\n",
            "they wo n't god afraid\n",
            "they 're not 2 and oh would well you home a screams\n",
            "but i 'm goin and you ca n't communicate if you 're part of\n",
            "they got ta do n't want\n",
            "************************************************************\n",
            "Artist: blink 182\n",
            "Title: all the small things\n",
            "Starting word: rage\n",
            "\n",
            "rage yo too everybody\n",
            "prayer is the grease by\n",
            "it 's beyond but did repeatx2\n",
            "yo else 's the workin\n",
            "the hand up looking and you will get him\n",
            "get me up a ride\n",
            "chorus to the chinese of the gaze\n",
            "beneath in the door\n",
            "the losing like the girls\n",
            "independent by speakers and shoot soir\n",
            "slick 's its and the sound are the tastes flow\n",
            "low of talking where i 've done at least .\n",
            "ak needs of\n",
            "************************************************************\n",
            "Artist: cardigans\n",
            "Title: lovefool\n",
            "Starting word: love\n",
            "\n",
            "love come up tonight\n",
            "mistletoe behind let\n",
            "and watch and emptiness no\n",
            "spark on the pavement as the peace\n",
            "just touch to the place\n",
            "where darkness of the names\n",
            "got to be their\n",
            "take my breath one of peace\n",
            "make a film car of someone\n",
            "the lights there was the names\n",
            "where the things that we could be part to find to a poor\n",
            "praying an chances when the phone\n",
            "keep them a manager way\n",
            "come much to\n",
            "************************************************************\n",
            "Artist: cardigans\n",
            "Title: lovefool\n",
            "Starting word: believe\n",
            "\n",
            "believe me weary let me\n",
            "if you loved know passed\n",
            "them folks and i 'm gone\n",
            "when you just a great to fade to find\n",
            "here here and down\n",
            "believe me mm-mm\n",
            "the snowbird in the faces\n",
            "all the broken and old shepherd i saw to live .\n",
            "where of this weekend not will be here\n",
            "when that flies but should\n",
            "your legs is all the id vision\n",
            "i 'll be rushing\n",
            "love was nice\n",
            "waits of wanted\n",
            "************************************************************\n",
            "Artist: cardigans\n",
            "Title: lovefool\n",
            "Starting word: rage\n",
            "\n",
            "rage\n",
            "join the red-nosed of rage in\n",
            "leave waits the molesta\n",
            "blame with my thugging\n",
            "would you live to suspended\n",
            "and waits i 'll survive a love was lately and me sure to the last california\n",
            "father around the pour in the coming\n",
            "want to be him here\n",
            "but sometimes we 'll see\n",
            "the beating fool to san\n",
            "am the began one of the power\n",
            "waitin that am one\n",
            "then over we country fittest fantasy\n",
            "but he talks\n",
            "************************************************************\n",
            "Artist: the bangles\n",
            "Title: eternal flame\n",
            "Starting word: love\n",
            "\n",
            "love goes all an chico sweet\n",
            "a laugh mistakes more more\n",
            "if that cartoon day knows\n",
            "the tracks then last you in mine\n",
            "all that way me nothing\n",
            "you 're my one destiny days\n",
            "but maybe it still so tits at my horse\n",
            "am the dry when you get away\n",
            "and i ca n't believe that 's there\n",
            "'cause i guess this is just all behind\n",
            "your back around a child face\n",
            "let me go this night\n",
            "let me\n",
            "************************************************************\n",
            "Artist: the bangles\n",
            "Title: eternal flame\n",
            "Starting word: believe\n",
            "\n",
            "believe i go it high\n",
            "it 's a breakin we got lost\n",
            "love in my fire\n",
            "walking my sooner to use\n",
            "never turn back to go on\n",
            "back around no stand on me\n",
            "do n't know i just straight\n",
            "a glimpse to make a fool\n",
            "'til stay\n",
            "so high i was sounding ' my way\n",
            "becomes my hand around jive\n",
            "window my heart\n",
            "call your back and whisper away\n",
            "forget her say i know\n",
            "that i should\n",
            "************************************************************\n",
            "Artist: the bangles\n",
            "Title: eternal flame\n",
            "Starting word: rage\n",
            "\n",
            "rage gun this by myself\n",
            "you 're gon na do my love with you\n",
            "but my heart ever give me no water fire\n",
            "so my love\n",
            "thank me to go\n",
            "but it 's you run\n",
            "does all alone\n",
            "and who say i want\n",
            "i need to see the light\n",
            "it 's getting your eyes\n",
            "you 're gone like a part way it\n",
            "but just for this girl while moment\n",
            "on the freeway buy\n",
            "baby things we would fall\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvv-1k7JJY38",
        "colab_type": "text"
      },
      "source": [
        "# 8. TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jhi2U-8Jb9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir name1:./log/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}